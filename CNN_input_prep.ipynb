{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage.io import imread, imshow\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from subprocess import check_output\n",
    "import os\n",
    "# GOOGLE COLAB: device = torch.device('cuda') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly load the train .jpg images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 60\n",
      "    Root location: sample_train_cervix_shapes/\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomResizedCrop(size=(256, 256), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "basepath = 'sample_train_cervix_shapes/'\n",
    "transform = transforms.Compose( [transforms.RandomResizedCrop(256),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# change from CIFAR: adding .ImageFolder (allows us to load images directly and allows us\n",
    "    # to transform images!)\n",
    "trainset_1 = datasets.ImageFolder(root=basepath,\n",
    "                                           transform=transform)\n",
    "    \n",
    "\n",
    "# IMPORTANT: train_loader needs to take a TENSOR as input\n",
    "train_loader_1 = torch.utils.data.DataLoader(trainset_1, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "print(trainset_1)\n",
    "print(type(train_loader_1))\n",
    "### iterating through a loader to see what its data looks like: ###\n",
    "# for i, (data, labels) in enumerate(train_loader):\n",
    "#     print(\"i\",i) --> starts at 0,  <class 'int'>\n",
    "#     print(\"data \", data) --> feature tensor,  <class 'int'>\n",
    "#     print(\"labels: \",labels) --> label tensor, <class 'list'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tensors instead of .jpg images in order to be able to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imagepath</th>\n",
       "      <th>filetype</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample_train_cervix_shapes/Type1/191.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample_train_cervix_shapes/Type1/201.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample_train_cervix_shapes/Type1/205.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample_train_cervix_shapes/Type1/208.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample_train_cervix_shapes/Type1/215.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  imagepath filetype type\n",
       "0  sample_train_cervix_shapes/Type1/191.jpg      jpg    1\n",
       "1  sample_train_cervix_shapes/Type1/201.jpg      jpg    1\n",
       "2  sample_train_cervix_shapes/Type1/205.jpg      jpg    1\n",
       "3  sample_train_cervix_shapes/Type1/208.jpg      jpg    1\n",
       "4  sample_train_cervix_shapes/Type1/215.jpg      jpg    1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 1. Create dataframe where you have each image path, file type and label: ###\n",
    "all_cervix_images = []\n",
    "\n",
    "for path in sorted(glob(basepath + \"*\")): # iterate through each directory (ie each type)\n",
    "    cervix_type = path.split(\"/\")[-1] # gives you the type (ie directory) of current instance\n",
    "    cervix_images = sorted(glob(basepath + cervix_type + \"/*\")) # gives you all images in current directory,\n",
    "        # in the following format: sample_train_cervix_shapes/Type1/image_number.jpg\n",
    "    all_cervix_images = all_cervix_images + cervix_images # list of all of the images in same format as previous!\n",
    "    \n",
    "all_cervix_images = pd.DataFrame({'imagepath': all_cervix_images})\n",
    "all_cervix_images['filetype'] = all_cervix_images.apply(lambda row: row.imagepath.split(\".\")[-1], axis=1)\n",
    "all_cervix_images['type'] = all_cervix_images.apply(lambda row: row.imagepath.split(\"/\")[-2], axis=1)\n",
    "\n",
    "### 2. Convert the 'type' column info to floats so that it can then be put into tensors! ###\n",
    "all_cervix_images.type[all_cervix_images.type == 'Type1'] = 1\n",
    "all_cervix_images.type[all_cervix_images.type == 'Type2'] = 2\n",
    "all_cervix_images.type[all_cervix_images.type == 'Type3'] = 3\n",
    "all_cervix_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. create feature tensor for each image ###\n",
    "\n",
    "def get_image_data(image_path):\n",
    "    \"\"\"\n",
    "    Method to get image data as np.array specifying image id and type\n",
    "    \"\"\"\n",
    "    #fname = get_filename(image_id, image_type)\n",
    "        # fname = sample_train_cervix_shapes/Type{1, 2, 3}/???.jpg\n",
    "    img = cv2.imread(image_path)\n",
    "        # cv2.imread(sample_train_cervix_shapes/Type{1, 2, 3}/???.jpg)\n",
    "    #assert img is not None, \"Failed to read image : %s, %s\" % (image_id, image_type)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def transform_image(img, rescaled_dim_x, rescaled_dim_y, to_gray=False, normalized=False):\n",
    "    resized = cv2.resize(img, (rescaled_dim_x, rescaled_dim_y), cv2.INTER_LINEAR)\n",
    "\n",
    "    if to_gray:\n",
    "        resized = cv2.cvtColor(resized, cv2.COLOR_RGB2GRAY).astype('float')\n",
    "    else:\n",
    "        resized = resized.astype('float')\n",
    "        \n",
    "    if normalized:\n",
    "        resized = cv2.normalize(resized, None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
    "        timg = resized.reshape(1, np.prod(resized.shape))\n",
    "        # return timg/np.linalg.norm(timg)\n",
    "    return resized\n",
    "\n",
    "complete_images = []\n",
    "import glob\n",
    "import os.path\n",
    "from os import path\n",
    "for file in list(glob.glob('sample_train_cervix_shapes/Type*/*')):\n",
    "    if file.find(\"Type1\")!= -1:\n",
    "        label = 1\n",
    "    if file.find(\"Type2\")!= -1:\n",
    "        label = 2\n",
    "    if file.find(\"Type3\")!= -1:\n",
    "        label = 3\n",
    "    complete_image = get_image_data(file)\n",
    "    complete_image = transform_image(complete_image, 256, 256, normalized=True)\n",
    "    complete_image = np.append(complete_image, label)\n",
    "    #complete_image = tf.convert_to_tensor(complete_image)\n",
    "    complete_images.append(complete_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#type(complete_images)\n",
    "#type(complete_images)\n",
    "#complete_images = np.asarray(complete_images)\n",
    "#type(all_cervix_images['type'])\n",
    "#import tensorflow as tf\n",
    "#complete_images = tf.convert_to_tensor(complete_images)\n",
    "#type(complete_images)\n",
    "#torch.tensor(complete_images)\n",
    "#labels_tensor = torch.tensor(all_cervix_images['type'])\n",
    "#type(labels_tensor)\n",
    "#trainset_2 = torch.cat([complete_images,labels_tensor],1)\n",
    "\n",
    "#labels = all_cervix_images['type']\n",
    "#print(type(complete_images))\n",
    "#print(type(labels))\n",
    "train_loader = torch.utils.data.DataLoader(complete_images, batch_size=4,shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.17670683, 0.15261044, 0.19277108, ..., 0.37751004, 0.37751004,\n",
       "        1.        ]),\n",
       " array([0.47058824, 0.45098039, 0.60784314, ..., 0.38039216, 0.35686275,\n",
       "        1.        ]),\n",
       " array([0.19211823, 0.13300493, 0.22167488, ..., 0.27586207, 0.44334975,\n",
       "        1.        ]),\n",
       " array([0.59349593, 0.32520325, 0.3902439 , ..., 0.29268293, 0.38617886,\n",
       "        1.        ]),\n",
       " array([0.02352941, 0.00784314, 0.00392157, ..., 0.        , 0.01568627,\n",
       "        1.        ]),\n",
       " array([1.        , 1.        , 1.        , ..., 0.15853659, 0.19512195,\n",
       "        1.        ]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.01568627, 0.01176471,\n",
       "        1.        ]),\n",
       " array([0.36507937, 0.28571429, 0.27777778, ..., 0.03968254, 0.07936508,\n",
       "        1.        ]),\n",
       " array([0.02352941, 0.01568627, 0.02745098, ..., 0.00392157, 0.02352941,\n",
       "        1.        ]),\n",
       " array([0.01960784, 0.01176471, 0.05098039, ..., 0.02352941, 0.05490196,\n",
       "        1.        ]),\n",
       " array([0.        , 0.00392157, 0.01176471, ..., 0.00784314, 0.        ,\n",
       "        1.        ]),\n",
       " array([0.852, 0.64 , 0.848, ..., 0.432, 0.616, 1.   ]),\n",
       " array([0.16470588, 0.2627451 , 0.32156863, ..., 0.20784314, 0.29411765,\n",
       "        1.        ]),\n",
       " array([0.00392157, 0.01176471, 0.01568627, ..., 0.01176471, 0.        ,\n",
       "        1.        ]),\n",
       " array([0.49382716, 0.40740741, 0.57613169, ..., 0.10699588, 0.21399177,\n",
       "        1.        ]),\n",
       " array([0.00392157, 0.00392157, 0.00392157, ..., 0.00784314, 0.00784314,\n",
       "        1.        ]),\n",
       " array([0.75903614, 0.51807229, 0.67068273, ..., 0.43373494, 0.54216867,\n",
       "        1.        ]),\n",
       " array([0.67058824, 0.4       , 0.57254902, ..., 0.08235294, 0.14509804,\n",
       "        1.        ]),\n",
       " array([0.00392157, 0.00392157, 0.00392157, ..., 0.        , 0.        ,\n",
       "        1.        ]),\n",
       " array([0.89328063, 0.70355731, 0.80632411, ..., 0.1541502 , 0.26086957,\n",
       "        1.        ]),\n",
       " array([0.08064516, 0.12096774, 0.13709677, ..., 0.22580645, 0.27016129,\n",
       "        2.        ]),\n",
       " array([0.01568627, 0.00784314, 0.01960784, ..., 0.00784314, 0.01960784,\n",
       "        2.        ]),\n",
       " array([0.72156863, 0.37647059, 0.50196078, ..., 0.03529412, 0.05882353,\n",
       "        2.        ]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.00392157, 0.00392157,\n",
       "        2.        ]),\n",
       " array([0.21115538, 0.15139442, 0.24302789, ..., 0.11952191, 0.23904382,\n",
       "        2.        ]),\n",
       " array([0.00392157, 0.00392157, 0.00392157, ..., 0.00784314, 0.00784314,\n",
       "        2.        ]),\n",
       " array([0., 0., 0., ..., 0., 0., 2.]),\n",
       " array([0.47058824, 0.36470588, 0.25882353, ..., 0.22352941, 0.14117647,\n",
       "        2.        ]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.00784314, 0.00392157,\n",
       "        2.        ]),\n",
       " array([0.95951417, 0.85020243, 0.89473684, ..., 0.63562753, 0.70040486,\n",
       "        2.        ]),\n",
       " array([0.57647059, 0.4627451 , 0.45490196, ..., 0.05882353, 0.        ,\n",
       "        2.        ]),\n",
       " array([0.00392157, 0.00392157, 0.00392157, ..., 0.        , 0.        ,\n",
       "        2.        ]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.00784314, 0.01176471,\n",
       "        2.        ]),\n",
       " array([0.52941176, 0.46606335, 0.30769231, ..., 0.61085973, 0.46153846,\n",
       "        2.        ]),\n",
       " array([0.81673307, 0.56972112, 0.77290837, ..., 0.12749004, 0.24302789,\n",
       "        2.        ]),\n",
       " array([0.00392157, 0.00784314, 0.        , ..., 0.        , 0.        ,\n",
       "        2.        ]),\n",
       " array([0.22916667, 0.22916667, 0.17083333, ..., 0.12916667, 0.05416667,\n",
       "        2.        ]),\n",
       " array([0.51898734, 0.3164557 , 0.45147679, ..., 0.16033755, 0.25738397,\n",
       "        2.        ]),\n",
       " array([0.4957265 , 0.34188034, 0.36752137, ..., 0.48290598, 0.42735043,\n",
       "        2.        ]),\n",
       " array([0.00784314, 0.        , 0.01176471, ..., 0.00392157, 0.01176471,\n",
       "        2.        ]),\n",
       " array([0., 0., 0., ..., 0., 0., 3.]),\n",
       " array([0.104, 0.232, 0.32 , ..., 0.152, 0.232, 3.   ]),\n",
       " array([0., 0., 0., ..., 0., 0., 3.]),\n",
       " array([0., 0., 0., ..., 0., 0., 3.]),\n",
       " array([0.48627451, 0.41568627, 0.40784314, ..., 0.24705882, 0.02352941,\n",
       "        3.        ]),\n",
       " array([0., 0., 0., ..., 0., 0., 3.]),\n",
       " array([0.00784314, 0.        , 0.01176471, ..., 0.        , 0.01568627,\n",
       "        3.        ]),\n",
       " array([0., 0., 0., ..., 0., 0., 3.]),\n",
       " array([0.00392157, 0.00392157, 0.00392157, ..., 0.        , 0.        ,\n",
       "        3.        ]),\n",
       " array([0.04313725, 0.        , 0.03137255, ..., 0.        , 0.01960784,\n",
       "        3.        ]),\n",
       " array([0., 0., 0., ..., 0., 0., 3.]),\n",
       " array([0.00392157, 0.00392157, 0.00392157, ..., 0.03137255, 0.04705882,\n",
       "        3.        ]),\n",
       " array([0.30864198, 0.30452675, 0.28395062, ..., 0.74897119, 0.66255144,\n",
       "        3.        ]),\n",
       " array([0.00784314, 0.00784314, 0.00784314, ..., 0.        , 0.        ,\n",
       "        3.        ]),\n",
       " array([0.7689243 , 0.60159363, 0.78486056, ..., 0.23904382, 0.35856574,\n",
       "        3.        ]),\n",
       " array([0.1254902 , 0.08627451, 0.0745098 , ..., 0.25882353, 0.40392157,\n",
       "        3.        ]),\n",
       " array([0.00392157, 0.00392157, 0.01176471, ..., 0.        , 0.01176471,\n",
       "        3.        ]),\n",
       " array([0.79912664, 0.43668122, 0.54148472, ..., 0.10480349, 0.12663755,\n",
       "        3.        ]),\n",
       " array([0.34024896, 0.26970954, 0.41078838, ..., 0.48547718, 0.51037344,\n",
       "        3.        ]),\n",
       " array([0.05098039, 0.03921569, 0.06666667, ..., 0.04313725, 0.07058824,\n",
       "        3.        ])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset\n",
    "#labels_tensor = torch.tensor(all_cervix_images['type'])\n",
    "#trainset = torch.cat([images_tensor,labels_tensor],1)\n",
    "#train_loader1 = torch.utils.data.DataLoader(trainset2, batch_size=4,\n",
    " #                                         shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "#for i, data in enumerate(train_loader, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    #print(data)\n",
    "#     inputs, labels = data\n",
    "   # print(inputs)\n",
    "#labels = all_cervix_images['type']\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "\n",
    "    super(Net, self).__init__()\n",
    "    #self.conv1 = nn.Conv2d(in_channels=3, out_channels=7, kernel_size=5)\n",
    "    #self.pool = nn.MaxPool2d(2,2)\n",
    "    #self.conv2 = nn.Conv2d(in_channels=7, out_channels=16, kernel_size=5)\n",
    "    self.linear1 = nn.Linear(256 * 256 * 3, 128) # 196608\n",
    "    self.linear2 = nn.Linear(128, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    # convolution layers\n",
    "    #x = self.pool(F.relu(self.conv1(x)))\n",
    "    #x = self.pool(F.relu(self.conv2(x)))\n",
    "    \n",
    "    # linear layer\n",
    "    #x = x.view(-1, 16 * 5 * 5) # flatten for linear layers\n",
    "    x = F.relu(self.linear1(x))\n",
    "    x = F.softmax(self.linear2(x), dim=1)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# create network depending on hardware available\n",
    "#if use_gpu:\n",
    " # net = Net().cuda()\n",
    "\n",
    "#else:\n",
    "net = Net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-77e36fd71621>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         '''\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-aa72958a69b4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#x = x.view(-1, 16 * 5 * 5) # flatten for linear layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1606\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtens_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1609\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    batch_size = 4\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        labels = []\n",
    "        inputs = []\n",
    "        for j in range(batch_size):\n",
    "            labels.append(data[j,-1])\n",
    "        for j in range(batch_size):\n",
    "            inputs.append(data[j,0:-1])\n",
    "            \n",
    "        ### safety check: ###\n",
    "            # print(\"len data\",len(data[0]))\n",
    "            # print(\"len inputs\",len(inputs[0]))\n",
    "            # len inputs = len data - 1 \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        '''\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "        '''\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
