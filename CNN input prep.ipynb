{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage.io import imread, imshow\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from subprocess import check_output\n",
    "import os\n",
    "# GOOGLE COLAB: device = torch.device('cuda') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly load the train .jpg images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose( [transforms.RandomResizedCrop(256),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# change from CIFAR: adding .ImageFolder (allows us to load images directly and allows us\n",
    "    # to transform images!)\n",
    "trainset_1 = datasets.ImageFolder(root=basepath,\n",
    "                                           transform=transform)\n",
    "    \n",
    "\n",
    "# IMPORTANT: train_loader needs to take a TENSOR as input\n",
    "train_loader_1 = torch.utils.data.DataLoader(trainset_1, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "### iterating through a loader to see what its data looks like: ###\n",
    "# for i, (data, labels) in enumerate(train_loader):\n",
    "#     print(\"i\",i) --> starts at 0,  <class 'int'>\n",
    "#     print(\"data \", data) --> feature tensor,  <class 'int'>\n",
    "#     print(\"labels: \",labels) --> label tensor, <class 'list'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tensors instead of .jpg images in order to be able to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imagepath</th>\n",
       "      <th>filetype</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample_train_cervix_shapes/Type1/191.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample_train_cervix_shapes/Type1/201.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample_train_cervix_shapes/Type1/205.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample_train_cervix_shapes/Type1/208.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample_train_cervix_shapes/Type1/215.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  imagepath filetype type\n",
       "0  sample_train_cervix_shapes/Type1/191.jpg      jpg    1\n",
       "1  sample_train_cervix_shapes/Type1/201.jpg      jpg    1\n",
       "2  sample_train_cervix_shapes/Type1/205.jpg      jpg    1\n",
       "3  sample_train_cervix_shapes/Type1/208.jpg      jpg    1\n",
       "4  sample_train_cervix_shapes/Type1/215.jpg      jpg    1"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 1. Create dataframe where you have each image path, file type and label: ###\n",
    "basepath = 'sample_train_cervix_shapes/'\n",
    "all_cervix_images = []\n",
    "\n",
    "for path in sorted(glob(basepath + \"*\")): # iterate through each directory (ie each type)\n",
    "    cervix_type = path.split(\"/\")[-1] # gives you the type (ie directory) of current instance\n",
    "    cervix_images = sorted(glob(basepath + cervix_type + \"/*\")) # gives you all images in current directory,\n",
    "        # in the following format: sample_train_cervix_shapes/Type1/image_number.jpg\n",
    "    all_cervix_images = all_cervix_images + cervix_images # list of all of the images in same format as previous!\n",
    "    \n",
    "all_cervix_images = pd.DataFrame({'imagepath': all_cervix_images})\n",
    "all_cervix_images['filetype'] = all_cervix_images.apply(lambda row: row.imagepath.split(\".\")[-1], axis=1)\n",
    "all_cervix_images['type'] = all_cervix_images.apply(lambda row: row.imagepath.split(\"/\")[-2], axis=1)\n",
    "\n",
    "### 2. Convert the 'type' column info to floats so that it can then be put into tensors! ###\n",
    "all_cervix_images.type[all_cervix_images.type == 'Type1'] = 1\n",
    "all_cervix_images.type[all_cervix_images.type == 'Type2'] = 2\n",
    "all_cervix_images.type[all_cervix_images.type == 'Type3'] = 3\n",
    "all_cervix_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "### 3. create feature tensor for each image ###\n",
    "\n",
    "def get_image_data(image_path):\n",
    "    \"\"\"\n",
    "    Method to get image data as np.array specifying image id and type\n",
    "    \"\"\"\n",
    "    #fname = get_filename(image_id, image_type)\n",
    "        # fname = sample_train_cervix_shapes/Type{1, 2, 3}/???.jpg\n",
    "    img = cv2.imread(image_path)\n",
    "        # cv2.imread(sample_train_cervix_shapes/Type{1, 2, 3}/???.jpg)\n",
    "    #assert img is not None, \"Failed to read image : %s, %s\" % (image_id, image_type)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "import tensorflow as tf\n",
    "complete_images = []\n",
    "import glob\n",
    "for file in list(glob.glob('sample_train_cervix_shapes/Type*/*')):\n",
    "    complete_image = get_image_data(file)\n",
    "    print(type(complete_image))\n",
    "    complete_image = tf.convert_to_tensor(complete_image)\n",
    "    print(type(complete_image))\n",
    "    complete_images.append(complete_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Shapes of all inputs must match: values[0].shape = [4128,3096,3] != values[4].shape = [3264,2448,3] [Op:Pack] name: packed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-376f8c1384dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#type(all_cervix_images['type'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#import tensorflow as tf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcomplete_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplete_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplete_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#torch.tensor(complete_images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1281\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1453\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cast_nested_seqs_to_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1455\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_autopacking_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"packed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[0;31m# checking.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dense_tensor_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_or_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_or_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m   \u001b[0mmust_pack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0mconverted_elems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   6335\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6336\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6337\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6338\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6339\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shapes of all inputs must match: values[0].shape = [4128,3096,3] != values[4].shape = [3264,2448,3] [Op:Pack] name: packed"
     ]
    }
   ],
   "source": [
    "type(complete_images)\n",
    "#type(complete_images)\n",
    "#complete_images = np.asarray(complete_images)\n",
    "#type(all_cervix_images['type'])\n",
    "#import tensorflow as tf\n",
    "complete_images = tf.convert_to_tensor(complete_images)\n",
    "type(complete_images)\n",
    "#torch.tensor(complete_images)\n",
    "#labels_tensor = torch.tensor(all_cervix_images['type'])\n",
    "#trainset_2 = torch.cat([complete_images,labels_tensor],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-68-32888dc4400a>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-68-32888dc4400a>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    shuffle=True, num_workers=2)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#labels_tensor = torch.tensor(all_cervix_images['type'])\n",
    "#trainset = torch.cat([images_tensor,labels_tensor],1)\n",
    "#train_loader1 = torch.utils.data.DataLoader(trainset2, batch_size=4,\n",
    " #                                         shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "# for i, data in enumerate(train_loader, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "#     print(data)\n",
    "#     inputs, labels = data\n",
    "#     print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
